{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3pgaqptvRWUNYW1FzNDhZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsaragih/RL-games/blob/main/RoGoU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install tensordict==0.2.0\n",
        "pip install torchrl==0.2.0"
      ],
      "metadata": {
        "id": "-ZLFuXfFWWXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9PQ_ZXS8EqDH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pygame\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os\n",
        "\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Env\n",
        "class Rogou(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
        "\n",
        "    def __init__(self, render_mode=None, starting_player=1):\n",
        "        self.window_size = 512  # The size of the PyGame window\n",
        "\n",
        "        # Observations are a box of 8 x 3, where each entry is either\n",
        "        # 0 (empty), >0 (cross), or <0 (circle)\n",
        "        self.observation_space = spaces.Box(-7, 7, shape=(3, 8), dtype=int)\n",
        "\n",
        "        # Agent has no control over # of steps. The agent can only\n",
        "        # choose to move one of the pieces that is not home yet.\n",
        "        self.action_space = spaces.Discrete(7)\n",
        "        self.starting_player = starting_player\n",
        "        self.player = starting_player # 1 for crosses, -1 for circles\n",
        "        self.prev_step = 0\n",
        "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # Expand first dimension of board and convert to float\n",
        "        board = np.expand_dims(self.board, axis=0).astype(float)\n",
        "        return self.board.copy()\n",
        "\n",
        "    def _get_info(self):\n",
        "        # Return number of rows that are not -1\n",
        "        return {\n",
        "            \"player\": self.player,\n",
        "            \"n_crosses\": np.count_nonzero(self.board > 0),\n",
        "            \"n_circles\": np.count_nonzero(self.board < 0),\n",
        "            \"crosses_home\": self.crosses_home,\n",
        "            \"circles_home\": self.circles_home,\n",
        "        }\n",
        "\n",
        "    def _check_win(self):\n",
        "        # Check if the current player has {self.size} in a row\n",
        "        # First check rows\n",
        "        for row in range(self.size):\n",
        "            if np.all(self.board[row, :] == self.player):\n",
        "                return True\n",
        "\n",
        "        # Now check columns\n",
        "        for col in range(self.size):\n",
        "            if np.all(self.board[:, col] == self.player):\n",
        "                return True\n",
        "\n",
        "        # Now check diagonals\n",
        "        if np.all(np.diag(self.board) == self.player):\n",
        "            return True\n",
        "        if np.all(np.diag(np.fliplr(self.board)) == self.player):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _get_step_size(self):\n",
        "        return self.board[0, 4] // 10\n",
        "\n",
        "    def _set_step_size(self):\n",
        "        step_size = np.random.choice(5, 1, p=[0.0625, 0.25, 0.375, 0.25, 0.0625])[0]\n",
        "        self.board[0, 4] = int(step_size) * 10\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # We need the following line to seed self.np_random\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Reset the crosses and circles locations\n",
        "        self.board = np.zeros((3, 8), dtype=int)\n",
        "        self.crosses_home = np.zeros(7, dtype=int)\n",
        "        self.circles_home = np.zeros(7, dtype=int)\n",
        "        self.player = self.starting_player\n",
        "        self._set_step_size()\n",
        "        self.prev_step = 0\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "    def _is_rosette(self, x, y):\n",
        "        if x == 0 and y == 0:\n",
        "            return True\n",
        "        elif x == 0 and y == 2:\n",
        "            return True\n",
        "        elif x == 6 and y == 0:\n",
        "            return True\n",
        "        elif x == 6 and y == 2:\n",
        "            return True\n",
        "        elif x == 3 and y == 1:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def _check_valid_square(self, x, y):\n",
        "        # If board[x, y] piece has the same sign as player, return False\n",
        "        # If board[x, y] piece has the opposite sign as player and is not on an immune square, return True\n",
        "        # If board[x, y] is empty return True\n",
        "        if (x == 4 or x == 5) and (y == 0 or y == 2):\n",
        "            return False\n",
        "        elif x < 0 or x > 7 or y < 0 or y > 2:\n",
        "            return False\n",
        "        elif self.board[y, x] == 0:\n",
        "            return True\n",
        "        # same sign as self.player\n",
        "        elif self.board[y, x] * self.player > 0:\n",
        "            return False\n",
        "        # opposite sign as self.player\n",
        "        elif self.board[y, x] * self.player < 0:\n",
        "            return not self._is_rosette(x, y)\n",
        "\n",
        "    def _is_home(self, x, y):\n",
        "        if self.player == 1:\n",
        "            return x == 5 and y == 0\n",
        "        else:\n",
        "            return x == 5 and y == 2\n",
        "\n",
        "    def _get_new_pos(self, curr_x, curr_y, step_size):\n",
        "        start_row = 0 if self.player == 1 else 2\n",
        "        if curr_x is None:\n",
        "            # Piece is not on the board\n",
        "            new_x = 4 - step_size\n",
        "            new_y = start_row\n",
        "            if self._check_valid_square(new_x, new_y):\n",
        "                return new_x, new_y\n",
        "\n",
        "        elif curr_x > 5 and curr_y == start_row:\n",
        "            new_x = curr_x - step_size\n",
        "            new_y = start_row\n",
        "            if new_x > 5 and self._check_valid_square(new_x, new_y):\n",
        "                return new_x, new_y\n",
        "            elif self._is_home(curr_x - step_size, curr_y):\n",
        "                # Piece is home\n",
        "                return -1, -1\n",
        "\n",
        "        elif curr_x < 4 and curr_y == start_row:\n",
        "            if curr_x - step_size >= 0:\n",
        "                new_x = curr_x - step_size\n",
        "                new_y = start_row\n",
        "            else:\n",
        "                # Piece makes it to the next row\n",
        "                new_x = step_size - curr_x - 1\n",
        "                new_y = 1\n",
        "            if self._check_valid_square(new_x, new_y):\n",
        "                return new_x, new_y\n",
        "\n",
        "        else:\n",
        "            new_x = curr_x + step_size\n",
        "            if new_x > 7 or new_x < 0:\n",
        "                new_x = 8 - (new_x - 7)\n",
        "                new_y = start_row\n",
        "            else:\n",
        "                new_y = 1\n",
        "\n",
        "            if self._check_valid_square(new_x, new_y):\n",
        "                return new_x, new_y\n",
        "            elif self._is_home(new_x, new_y):\n",
        "                # Piece is home\n",
        "                return -1, -1\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def _check_possible_move(self, id, step_size):\n",
        "        # print(\"Curr y, x: \", np.where(self.board == id))\n",
        "        curr_y, curr_x = np.where(self.board == id)\n",
        "        if curr_x.size == 0:\n",
        "            curr_x, curr_y = None, None\n",
        "        else:\n",
        "            curr_x, curr_y = curr_x[0], curr_y[0]\n",
        "        new_x, new_y = self._get_new_pos(curr_x, curr_y, step_size)\n",
        "        # print(\"New position: \", new_y, new_x )\n",
        "        return new_x is not None, curr_x, curr_y, new_x, new_y\n",
        "\n",
        "    def step(self, action):\n",
        "        # Action is the id of the piece to move\n",
        "        assert self.action_space.contains(action)\n",
        "        # Note that a player must move a piece if possible\n",
        "\n",
        "        home_list = self.crosses_home if self.player == 1 else self.circles_home\n",
        "        chose_impossible = home_list[action]\n",
        "\n",
        "        # assert not home_list[action]\n",
        "        step_size = self._get_step_size()\n",
        "\n",
        "        id = self.player * (action + 1)\n",
        "        if step_size == 0:\n",
        "            # Piece is not on the board\n",
        "            is_possible, curr_x, curr_y, new_x, new_y = True, None, None, None, None\n",
        "        elif not chose_impossible:\n",
        "            is_possible, curr_x, curr_y, new_x, new_y = self._check_possible_move(id, step_size)\n",
        "        else:\n",
        "            is_possible, curr_x, curr_y, new_x, new_y = False, None, None, None, None\n",
        "\n",
        "        if not is_possible:\n",
        "            # Unable to move\n",
        "            i = 0\n",
        "            # List of indices not home for current player\n",
        "            players = np.where(home_list == 0)[0]\n",
        "            np.random.shuffle(players)\n",
        "            for i in players:\n",
        "                id = self.player * (i + 1)\n",
        "                is_possible, curr_x, curr_y, new_x, new_y = self._check_possible_move(id, step_size)\n",
        "                if is_possible:\n",
        "                    break\n",
        "            if not is_possible:\n",
        "                # No possible moves\n",
        "                # print(f\"Player {self.player} has no possible moves.\")\n",
        "                # print(f\"Step size: {step_size}.\")\n",
        "                self.prev_step = 0\n",
        "                self._set_step_size()\n",
        "                self.player = -self.player\n",
        "                return self._get_obs(), 0, False, False, self._get_info()\n",
        "            else:\n",
        "                chose_impossible = True\n",
        "\n",
        "        # print(f\"Player {self.player} chose piece with id {id} and step size {step_size}.\")\n",
        "        # print(f\"Current position: ({curr_y}, {curr_x}). New position: ({new_y}, {new_x}).\")\n",
        "        if is_possible:\n",
        "            if curr_x is not None and curr_y is not None:\n",
        "                self.board[curr_y, curr_x] = 0\n",
        "\n",
        "            if new_x == -1 and new_y == -1:\n",
        "                # Piece is home\n",
        "                home_list[action] = 1\n",
        "            elif new_x is not None and new_y is not None:\n",
        "                # Piece moved\n",
        "                self.board[new_y, new_x] = id\n",
        "\n",
        "\n",
        "        # An episode is done iff the current player has won or the board is full\n",
        "        terminated = home_list.sum() == 7\n",
        "\n",
        "        # Reward 1, 0, -1 for win, draw, loss\n",
        "        s = 1 if self.player == self.starting_player else -1\n",
        "        if terminated:\n",
        "            reward = 10 * s\n",
        "        elif not chose_impossible and home_list[action] == 1:\n",
        "            # Got a piece home\n",
        "            reward = 1 * s\n",
        "        elif chose_impossible:\n",
        "            reward = -0.1 * s\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        if not self._is_rosette(new_x, new_y):\n",
        "            self.player = -self.player\n",
        "        else:\n",
        "            # print(\"Player gets another turn.\")\n",
        "            pass\n",
        "\n",
        "        self.prev_step = step_size\n",
        "        self._set_step_size()\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, reward, terminated, False, info\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode == \"rgb_array\":\n",
        "            return self._render_frame()\n",
        "\n",
        "    def _draw_cross(self, canvas, center_x, center_y, size, color, line_width=3):\n",
        "        # Draw a cross centered at (center_x, center_y) with a given size and color\n",
        "        pad = size / 10\n",
        "        pygame.draw.line(\n",
        "            canvas,\n",
        "            color,\n",
        "            (center_x - size / 2 + pad, center_y - size / 2 + pad),\n",
        "            (center_x + size / 2 - pad, center_y + size / 2 - pad),\n",
        "            width=line_width,\n",
        "        )\n",
        "        pygame.draw.line(\n",
        "            canvas,\n",
        "            color,\n",
        "            (center_x + size / 2 - pad, center_y - size / 2 + pad),\n",
        "            (center_x - size / 2 + pad, center_y + size / 2 - pad),\n",
        "            width=line_width,\n",
        "        )\n",
        "\n",
        "    def _draw_circle(self, canvas, center_x, center_y, size, color, line_width=3):\n",
        "        # Draw a circle centered at (center_x, center_y) with a given size and color\n",
        "        pad = size / 10\n",
        "        pygame.draw.circle(\n",
        "            canvas,\n",
        "            color,\n",
        "            (center_x, center_y),\n",
        "            int(size / 2 - pad),\n",
        "            width=line_width,\n",
        "        )\n",
        "\n",
        "\n",
        "    def _render_frame(self):\n",
        "        if self.window is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
        "        if self.clock is None and self.render_mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        pix_square_size = (\n",
        "            self.window_size / 8\n",
        "        )  # The size of a single grid square in pixels\n",
        "\n",
        "        # First we draw crosses and circles\n",
        "        pad = pix_square_size / 10\n",
        "        width = 3\n",
        "\n",
        "        # Paint black rectangles at x == 4, 5 and y == 0, 2\n",
        "        pygame.draw.rect(\n",
        "            canvas,\n",
        "            (0, 0, 0),\n",
        "            (pix_square_size * 4, pix_square_size * 3, pix_square_size * 2, pix_square_size),\n",
        "            width=0,\n",
        "        )\n",
        "        pygame.draw.rect(\n",
        "            canvas,\n",
        "            (0, 0, 0),\n",
        "            (pix_square_size * 4, pix_square_size * 5, pix_square_size * 2, pix_square_size),\n",
        "            width=0,\n",
        "        )\n",
        "\n",
        "        rosette_coords = [(0, 0), (0, 2), (6, 0), (6, 2), (3, 1)]\n",
        "        for x, y in rosette_coords:\n",
        "            # Paint flower if a rosette square\n",
        "            pygame.draw.circle(\n",
        "                canvas,\n",
        "                (255, 0, 0),\n",
        "                (\n",
        "                    int(pix_square_size * (x + 0.5)),\n",
        "                    int(pix_square_size * (y + 3 + 0.5)),\n",
        "                ),\n",
        "                int(pix_square_size / 4 - pad),\n",
        "                width=width,\n",
        "            )\n",
        "\n",
        "        for row in range(3, 6):\n",
        "            for col in range(8):\n",
        "                idx_row = row - 3\n",
        "                if self.board[idx_row, col] > 0:\n",
        "                    self._draw_cross(\n",
        "                        canvas,\n",
        "                        int(pix_square_size * (col + 0.5)),\n",
        "                        int(pix_square_size * (row + 0.5)),\n",
        "                        int(pix_square_size),\n",
        "                        (0, 0, 0),\n",
        "                    )\n",
        "                elif self.board[idx_row, col] < 0:\n",
        "                    self._draw_circle(\n",
        "                        canvas,\n",
        "                        int(pix_square_size * (col + 0.5)),\n",
        "                        int(pix_square_size * (row + 0.5)),\n",
        "                        int(pix_square_size),\n",
        "                        (0, 0, 0),\n",
        "                    )\n",
        "\n",
        "        # Finally, add some gridlines\n",
        "        # Note x: 0 is the leftmost, y: 0 is the topmost\n",
        "        for y in range(3, 7):\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (0, pix_square_size * y),\n",
        "                (self.window_size, pix_square_size * y),\n",
        "                width=3,\n",
        "            )\n",
        "        for x in range(9):\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (pix_square_size * x, pix_square_size * 3),\n",
        "                (pix_square_size * x, pix_square_size * 6),\n",
        "                width=3,\n",
        "            )\n",
        "\n",
        "        crosses_left = 7 - self.crosses_home.sum() - np.count_nonzero(self.board > 0)\n",
        "        circles_left = 7 - self.circles_home.sum() - np.count_nonzero(self.board < 0)\n",
        "        for i in range(crosses_left):\n",
        "            self._draw_cross(\n",
        "                canvas,\n",
        "                int(0.3 * pix_square_size * (i + 0.5) + 10),\n",
        "                int(2 * pix_square_size * (0.5)),\n",
        "                int(pix_square_size / 4),\n",
        "                (0, 0, 0),\n",
        "            )\n",
        "        for i in range(circles_left):\n",
        "            self._draw_circle(\n",
        "                canvas,\n",
        "                int(0.25 * pix_square_size * (i + 0.5) + 340),\n",
        "                int(2 * pix_square_size * (0.5)),\n",
        "                int(pix_square_size / 4),\n",
        "                (0, 0, 0),\n",
        "            )\n",
        "\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "\n",
        "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
        "            # The following line will automatically add a delay to keep the framerate stable.\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        else:  # rgb_array\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()"
      ],
      "metadata": {
        "id": "kgGWN46fE6Ih"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Neural Net\n",
        "class PlayerNet(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    h, w = input_dim\n",
        "\n",
        "    self.step_encoder = nn.Linear(1, 64)\n",
        "    self.online = self._build_dense(h, w, output_dim)\n",
        "    self.target = self._build_dense(h, w, output_dim)\n",
        "\n",
        "    self.target.load_state_dict(self.online.state_dict())\n",
        "\n",
        "    for p in self.target.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "  def forward(self, input, model):\n",
        "    input = input.float()\n",
        "    nn_model = self.online if model == \"online\" else self.target\n",
        "    step_batch = input[:, 0, 4].contiguous() / 10\n",
        "    input[:, 0, 4] = 0.\n",
        "\n",
        "    if len(step_batch.shape) == 1:\n",
        "      step_batch = step_batch.unsqueeze(1)\n",
        "    step_emb = self.step_encoder(step_batch)\n",
        "\n",
        "    for i, layer in enumerate(nn_model):\n",
        "      if i == 3:\n",
        "        input = torch.cat([input, step_emb], dim=1)\n",
        "      input = layer(input)\n",
        "    return input\n",
        "\n",
        "  def _build_cnn(self, h, w, output_dim):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=4, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear((h-4)** 2 * 64, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, output_dim),\n",
        "    )\n",
        "\n",
        "  def _build_dense(self, h, w, output_dim):\n",
        "    return nn.ModuleList([\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(h * w, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, output_dim)]\n",
        "    )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tN5JqY2uWuSn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model class\n",
        "class Player:\n",
        "  def __init__(self, player, state_dim, action_dim, save_dir):\n",
        "    # player = 1 (X) or -1 (O)\n",
        "    # state_dim = (3, 8)\n",
        "    # action_dim = 7\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim # (x, y) to place the X or O\n",
        "    self.player = player\n",
        "    self.save_dir = save_dir\n",
        "\n",
        "    self.exp_rate = 1\n",
        "    self.exp_rate_decay = 0.9999975\n",
        "    self.exp_rate_min = 0.2\n",
        "\n",
        "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    self.net = PlayerNet(self.state_dim, self.action_dim).float()\n",
        "    self.net = self.net.to(device=self.device)\n",
        "\n",
        "    self.curr_step = 0\n",
        "    self.save_every = 5e4\n",
        "\n",
        "    self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000,\n",
        "                                         device=torch.device('cpu')))\n",
        "    self.batch_size = 32\n",
        "\n",
        "    self.gamma = 0.9\n",
        "    self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.001)\n",
        "    self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    self.burnin = 1e3  # min. experiences before training\n",
        "    self.learn_every = 1  # no. of experiences between updates to Q_online\n",
        "    self.sync_every = 1e3  # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "\n",
        "  def act(self, state):\n",
        "    # State is 3 x 8\n",
        "    # Explore\n",
        "    if np.random.rand() < self.exp_rate:\n",
        "      # We have 7 pieces. Moreover, with how the env is setup, if we choose\n",
        "      # an invalid piece, it will automatically force a random move if possible.\n",
        "      action_idx = np.random.randint(7)\n",
        "      from_model = False\n",
        "    else:\n",
        "      state = state.__array__().copy()\n",
        "      state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "      action_values = self.net(state, \"online\")\n",
        "\n",
        "      # Take the argmax over the valid action values\n",
        "      action_idx = torch.argmax(action_values, dim=1).item()\n",
        "\n",
        "      from_model = True\n",
        "\n",
        "    self.exp_rate *= self.exp_rate_decay\n",
        "    self.exp_rate = max(self.exp_rate_min, self.exp_rate)\n",
        "\n",
        "    self.curr_step += 1\n",
        "    return action_idx, from_model\n",
        "\n",
        "  def cache(self, state, next_state, action, reward, done):\n",
        "    # Preprocess as before and note that we convert to array before tensor-ing\n",
        "    def first_if_tuple(x):\n",
        "      return x[0] if isinstance(x, tuple) else x\n",
        "\n",
        "    state = first_if_tuple(state).__array__() # size x size\n",
        "    next_state = first_if_tuple(state).__array__()\n",
        "    # print(state)\n",
        "    state = torch.tensor(state.copy())\n",
        "    next_state = torch.tensor(next_state.copy())\n",
        "    action = torch.tensor([action])\n",
        "    reward = torch.tensor([reward])\n",
        "    done = torch.tensor([done])\n",
        "\n",
        "    # Leave batch size unspecified (later fixed during sampling)\n",
        "    self.memory.add(TensorDict({\n",
        "        'state': state,\n",
        "        'next_state': next_state,\n",
        "        'action': action,\n",
        "        'reward': reward,\n",
        "        'done': done\n",
        "    }, batch_size=[]))\n",
        "\n",
        "  def recall(self):\n",
        "    batch = self.memory.sample(self.batch_size).to(self.device)\n",
        "    state, next_state, action, reward, done = \\\n",
        "     (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
        "    return state, next_state, action, reward, done\n",
        "\n",
        "  def td_estimate(self, state, action):\n",
        "    current_Q = self.net(state, model=\"online\")\n",
        "    # print(current_Q.shape)\n",
        "    # print(\"State: \", state)\n",
        "    # print(\"Action: \", action)\n",
        "    action_clone = action.contiguous()\n",
        "    # print(\"Action print: \", action_clone)\n",
        "    current_Q = current_Q[np.arange(0, self.batch_size), action_clone]\n",
        "    # print(current_Q.shape)\n",
        "    return current_Q\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def td_target(self, reward, next_state, done):\n",
        "    next_state_Q = self.net(next_state, model=\"online\")\n",
        "    max_action = torch.argmax(next_state_Q, dim=1)\n",
        "    next_Q = self.net(next_state, model=\"target\")[\n",
        "        np.arange(0, self.batch_size), max_action\n",
        "    ]\n",
        "\n",
        "    return (reward + (1 - done.float()) * (self.gamma * next_Q)).float()\n",
        "\n",
        "  def update_Q_online(self, td_e, td_t):\n",
        "    loss = self.loss_fn(td_e, td_t)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "  def sync_Q_target(self):\n",
        "    self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "\n",
        "  def learn(self):\n",
        "    if self.curr_step % self.sync_every == 0:\n",
        "      self.sync_Q_target()\n",
        "    if self.curr_step % self.save_every == 0:\n",
        "      self.save()\n",
        "    if self.curr_step % self.learn_every != 0:\n",
        "      return None, None\n",
        "    if self.curr_step < self.burnin:\n",
        "      return None, None\n",
        "\n",
        "    # Sample\n",
        "    state, next_state, action, reward, done = self.recall()\n",
        "    if state is None or action is None:\n",
        "      return None, None\n",
        "\n",
        "    # Get td_e, td_t\n",
        "    td_e = self.td_estimate(state, action)\n",
        "    td_t = self.td_target(reward, next_state, done)\n",
        "\n",
        "    # Update params\n",
        "    loss = self.update_Q_online(td_e, td_t)\n",
        "\n",
        "    return (td_e.mean().item()), loss\n",
        "\n",
        "\n",
        "  def save(self):\n",
        "    pre = \"X\" if self.player == 1 else \"O\"\n",
        "    save_path = (\n",
        "        self.save_dir / f\"{pre}_player_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "    )\n",
        "    torch.save(\n",
        "        dict(model=self.net.state_dict(), exploration_rate=self.exp_rate),\n",
        "        save_path,\n",
        "    )\n",
        "    print(f\"PlayerNet saved to {save_path} at step {self.curr_step}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GOaXgi21Wy4i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Logger\n",
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir, pre=\"W\"):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / f\"{pre}_reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / f\"{pre}_length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / f\"{pre}_loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / f\"{pre}_q_plot.jpg\"\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        look_behind = 50\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-look_behind:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-look_behind:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-look_behind:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-look_behind:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
        "            plt.clf()\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
        "            plt.legend()\n",
        "            plt.savefig(getattr(self, f'{metric}_plot'))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6kseWgeUW4md"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "env = Rogou(render_mode='rgb_array')\n",
        "\n",
        "# Small technicality\n",
        "def preprocess(state):\n",
        "  return np.expand_dims(state, axis=0).astype(np.float32)\n",
        "\n",
        "def swap(state):\n",
        "  step = state[0, 4]\n",
        "  state = np.flipud(state * -1)\n",
        "  state[2, 4] = 0\n",
        "  state[0, 4] = step\n",
        "  return state\n",
        "\n",
        "save_dir = Path(\"tcheckpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "player = 1\n",
        "model = Player(player=player, state_dim=(3, 8), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "episodes = 2000\n",
        "def train_loop(episodes):\n",
        "    logger = MetricLogger(save_dir)\n",
        "    step_count = 0\n",
        "    for e in range(episodes):\n",
        "\n",
        "        state, info = env.reset()\n",
        "        ep_steps = 0\n",
        "        # Play the game!\n",
        "        # state = preprocess(state)\n",
        "        # Self-play\n",
        "        while True:\n",
        "            flip = info[\"player\"] * player # -1 if swap necessary\n",
        "            # Run agent on the state\n",
        "            # Multiply state by playing_now so that the model only sees itself\n",
        "            # as playing 'X'.\n",
        "            if flip == -1:\n",
        "              state = swap(state)\n",
        "            # print(\"Prev state\\n\", state)\n",
        "            # print(\"What env sees\\n\", env._get_obs())\n",
        "\n",
        "            action, f = model.act(state)\n",
        "\n",
        "            next_state, reward, done, trunc, info = env.step(action)\n",
        "            # next_state = preprocess(next_state)\n",
        "            if flip == -1:\n",
        "              next_state = swap(next_state)\n",
        "            # print(\"Next state\\n\", next_state)\n",
        "            reward *= flip # +1 reward whenever the model wins (either X or O)\n",
        "\n",
        "            # Remember\n",
        "            model.cache(state, next_state, action, reward, done)\n",
        "\n",
        "            # Learn\n",
        "            q, loss = model.learn()\n",
        "\n",
        "            # Logging\n",
        "            logger.log_step(reward, loss, q)\n",
        "\n",
        "            # Update variables\n",
        "            state = swap(next_state) if flip == -1 else next_state\n",
        "            ep_steps += 1\n",
        "            step_count += 1\n",
        "\n",
        "            # Check if end of game\n",
        "            if done:\n",
        "                # print(\"curr_reward: \", reward)\n",
        "                # print(\"Game length \", ep_steps)\n",
        "                # print(\"Overall Steps \", step_count)\n",
        "                break\n",
        "\n",
        "        logger.log_episode()\n",
        "\n",
        "        if (e % 20 == 0) or (e == episodes - 1):\n",
        "            logger.record(episode=e, epsilon=model.exp_rate, step=model.curr_step)\n",
        "\n",
        "train_loop(episodes)\n",
        "# train_loop(episodes, \"O\")"
      ],
      "metadata": {
        "id": "4ucgLWTaW761"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def test_env(k: int=1):\n",
        "  env = Rogou(render_mode='rgb_array')\n",
        "  state, info = env.reset()\n",
        "  screen = env.render()\n",
        "  images = [Image.fromarray(screen)]\n",
        "  terminated = False\n",
        "  i = 0\n",
        "  while not terminated:\n",
        "      if i == 18: break\n",
        "      curr_player = info[\"player\"]\n",
        "      # if i % 2 == 0:\n",
        "      #   action, fn = model.act(state)\n",
        "      # else:\n",
        "      #   action = env.action_space.sample()\n",
        "      if curr_player == -1:\n",
        "        state = swap(state)\n",
        "      action, fn = model.act(state)\n",
        "      state, reward, terminated, truncated, info = env.step(action)\n",
        "      # Render screen every k steps\n",
        "      if i % k == 0:\n",
        "        # print(f'observation \\n{state}')\n",
        "        screen = env.render()\n",
        "        images.append(Image.fromarray(screen))\n",
        "      if terminated:\n",
        "          observation, info = env.reset()\n",
        "\n",
        "      if terminated or truncated:\n",
        "        print(\"DONE \", i)\n",
        "        print(\"reward: \", reward)\n",
        "        break\n",
        "      i += 1\n",
        "\n",
        "\n",
        "  env.close()\n",
        "\n",
        "  return images\n",
        "\n",
        "# Save GIF image\n",
        "images = test_env()\n",
        "image_file = 'short.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:],  loop=0, duration=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgHCU9hOExP-",
        "outputId": "1975b3a4-4cb3-4dab-c895-01048e5f4f04"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the current model\n",
        "\n",
        "torch.save(\n",
        "        dict(model=model.net.state_dict(), exploration_rate=model.exp_rate),\n",
        "        \"model_2000.chkpt\",\n",
        ")"
      ],
      "metadata": {
        "id": "i3fE1q0KZnlK"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}